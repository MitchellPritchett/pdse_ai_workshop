{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[PDSE]AI_workshop_week6(with_answer).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPJIVDdH7rOqmesu1hkfu8h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MitchellPritchett/pdse_ai_workshop/blob/main/%5BPDSE%5DAI_workshop_week6(with_answer).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#기본적인 forward, backward layer 모두 만들어보기"
      ],
      "metadata": {
        "id": "RcgEaRXiH9b-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5,6주차\n"
      ],
      "metadata": {
        "id": "hr4k8VBX0-Dr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Affine Layer"
      ],
      "metadata": {
        "id": "Ll4_vnG41KVY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfSe2Oro0Vej"
      },
      "outputs": [],
      "source": [
        "from builtins import range\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rel_error(x, y):\n",
        "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
      ],
      "metadata": {
        "id": "X5LJWN3vH5Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from builtins import range\n",
        "from past.builtins import xrange\n",
        "\n",
        "import numpy as np\n",
        "from random import randrange\n",
        "\n",
        "\n",
        "def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n",
        "    \"\"\"\n",
        "    a naive implementation of numerical gradient of f at x\n",
        "    - f should be a function that takes a single argument\n",
        "    - x is the point (numpy array) to evaluate the gradient at\n",
        "    \"\"\"\n",
        "\n",
        "    fx = f(x)  # evaluate function value at original point\n",
        "    grad = np.zeros_like(x)\n",
        "    # iterate over all indexes in x\n",
        "    it = np.nditer(x, flags=[\"multi_index\"], op_flags=[\"readwrite\"])\n",
        "    while not it.finished:\n",
        "\n",
        "        # evaluate function at x+h\n",
        "        ix = it.multi_index\n",
        "        oldval = x[ix]\n",
        "        x[ix] = oldval + h  # increment by h\n",
        "        fxph = f(x)  # evalute f(x + h)\n",
        "        x[ix] = oldval - h\n",
        "        fxmh = f(x)  # evaluate f(x - h)\n",
        "        x[ix] = oldval  # restore\n",
        "\n",
        "        # compute the partial derivative with centered formula\n",
        "        grad[ix] = (fxph - fxmh) / (2 * h)  # the slope\n",
        "        if verbose:\n",
        "            print(ix, grad[ix])\n",
        "        it.iternext()  # step to next dimension\n",
        "\n",
        "    return grad\n",
        "\n",
        "\n",
        "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
        "    \"\"\"\n",
        "    Evaluate a numeric gradient for a function that accepts a numpy\n",
        "    array and returns a numpy array.\n",
        "    \"\"\"\n",
        "    grad = np.zeros_like(x)\n",
        "    it = np.nditer(x, flags=[\"multi_index\"], op_flags=[\"readwrite\"])\n",
        "    while not it.finished:\n",
        "        ix = it.multi_index\n",
        "\n",
        "        oldval = x[ix]\n",
        "        x[ix] = oldval + h\n",
        "        pos = f(x).copy()\n",
        "        x[ix] = oldval - h\n",
        "        neg = f(x).copy()\n",
        "        x[ix] = oldval\n",
        "\n",
        "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
        "        it.iternext()\n",
        "    return grad"
      ],
      "metadata": {
        "id": "AsiAUU_6HVc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def affine_forward(x, w, b):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for an affine (fully-connected) layer.\n",
        "\n",
        "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
        "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
        "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
        "    then transform it to an output vector of dimension M.\n",
        "\n",
        "    Inputs:\n",
        "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
        "    - w: A numpy array of weights, of shape (D, M)\n",
        "    - b: A numpy array of biases, of shape (M,)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, M)\n",
        "    - cache: (x, w, b)\n",
        "    \"\"\"\n",
        "    out = None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the affine forward pass. Store the result in out. You   #\n",
        "    # will need to reshape the input into rows.                               #\n",
        "    ###########################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    \n",
        "    x_tmp = x.reshape(x.shape[0], -1)\n",
        "    out = x_tmp.dot(w) + b\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = (x, w, b)\n",
        "    return out, cache"
      ],
      "metadata": {
        "id": "M3Jk4i3q0etT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.random.rand(2,3,2,2) # understanding numpy.reshape()"
      ],
      "metadata": {
        "id": "ZJfNEPskKH0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A # understanding numpy.reshape()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByvC63WTKMCT",
        "outputId": "cf94b918-8bdb-46a3-bf2d-d6f9eec2cc94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[0.36166074, 0.09931634],\n",
              "         [0.14442726, 0.39309712]],\n",
              "\n",
              "        [[0.97988929, 0.0540233 ],\n",
              "         [0.54649524, 0.48413006]],\n",
              "\n",
              "        [[0.13342669, 0.85296452],\n",
              "         [0.71177086, 0.57039255]]],\n",
              "\n",
              "\n",
              "       [[[0.74630582, 0.50751086],\n",
              "         [0.204662  , 0.61281788]],\n",
              "\n",
              "        [[0.10267598, 0.30841572],\n",
              "         [0.69959224, 0.08245966]],\n",
              "\n",
              "        [[0.35603975, 0.3663921 ],\n",
              "         [0.4870731 , 0.50307399]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.reshape(2,3,-1) # understanding numpy.reshape()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp7iMBZDKSAp",
        "outputId": "7d35140c-f440-4050-f363-3c74f147e953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.36166074, 0.09931634, 0.14442726, 0.39309712],\n",
              "        [0.97988929, 0.0540233 , 0.54649524, 0.48413006],\n",
              "        [0.13342669, 0.85296452, 0.71177086, 0.57039255]],\n",
              "\n",
              "       [[0.74630582, 0.50751086, 0.204662  , 0.61281788],\n",
              "        [0.10267598, 0.30841572, 0.69959224, 0.08245966],\n",
              "        [0.35603975, 0.3663921 , 0.4870731 , 0.50307399]]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the affine_forward function\n",
        "\n",
        "num_inputs = 2\n",
        "input_shape = (4, 5, 6)\n",
        "output_dim = 3\n",
        "\n",
        "input_size = num_inputs * np.prod(input_shape)\n",
        "weight_size = output_dim * np.prod(input_shape)\n",
        "\n",
        "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
        "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
        "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
        "\n",
        "out, _ = affine_forward(x, w, b)\n",
        "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
        "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
        "\n",
        "# Compare your output with ours. The error should be around e-9 or less.\n",
        "print('Testing affine_forward function:')\n",
        "print('difference: ', rel_error(out, correct_out))"
      ],
      "metadata": {
        "id": "C5dN2Fos0lHO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d7b1232-e3f3-4057-9dcc-5b071bc7e254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing affine_forward function:\n",
            "difference:  9.769849468192957e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def affine_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for an affine layer.\n",
        "\n",
        "    Inputs:\n",
        "    - dout: Upstream derivative, of shape (N, M)\n",
        "    - cache: Tuple of:\n",
        "      - x: Input data, of shape (N, d_1, ... d_k)\n",
        "      - w: Weights, of shape (D, M)\n",
        "      - b: Biases, of shape (M,)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
        "    - dw: Gradient with respect to w, of shape (D, M)\n",
        "    - db: Gradient with respect to b, of shape (M,)\n",
        "    \"\"\"\n",
        "    x, w, b = cache\n",
        "    dx, dw, db = None, None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the affine backward pass.                               #\n",
        "    ###########################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    dx = dout.dot(w.T).reshape(x.shape) # dout(N,M)과 w(D,M).T ==> dx(N,D), nparray.T \n",
        "    dw = x.reshape(x.shape[0], -1).T.dot(dout)# dout(N,M)과 x(N,D).T ==> dw(D,M)\n",
        "    db = dout.sum(axis=0) #일까요? # b의 shape으로 sum, dout.shape == (N,M), db == (M,)\n",
        "    # np.sum(axis=0또는1)\n",
        "    \n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx, dw, db"
      ],
      "metadata": {
        "id": "AsLFkw1z0YNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the affine_backward function\n",
        "np.random.seed(231)\n",
        "x = np.random.randn(10, 2, 3)\n",
        "w = np.random.randn(6, 5)\n",
        "b = np.random.randn(5)\n",
        "dout = np.random.randn(10, 5)\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
        "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
        "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
        "\n",
        "_, cache = affine_forward(x, w, b)\n",
        "dx, dw, db = affine_backward(dout, cache)\n",
        "\n",
        "# The error should be around e-10 or less\n",
        "print('Testing affine_backward function:')\n",
        "print('dx error: ', rel_error(dx_num, dx))\n",
        "print('dw error: ', rel_error(dw_num, dw))\n",
        "print('db error: ', rel_error(db_num, db))"
      ],
      "metadata": {
        "id": "v96UgC920pPL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd48349c-1d38-4511-eb28-e6d56f4ed730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing affine_backward function:\n",
            "dx error:  5.399100368651805e-11\n",
            "dw error:  9.904211865398145e-11\n",
            "db error:  2.4122867568119087e-11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7주차"
      ],
      "metadata": {
        "id": "ZBg0d6PxHzZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReLU Activation"
      ],
      "metadata": {
        "id": "y7QB3TUC1ONX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_forward(x):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - x: Inputs, of any shape\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output, of the same shape as x\n",
        "    - cache: x\n",
        "    \"\"\"\n",
        "    out = None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU forward pass.                                  #\n",
        "    ###########################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    pass\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = x\n",
        "    return out, cache"
      ],
      "metadata": {
        "id": "5nWRVHew0bY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the relu_forward function\n",
        "\n",
        "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
        "\n",
        "out, _ = relu_forward(x)\n",
        "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
        "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
        "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
        "\n",
        "# Compare your output with ours. The error should be on the order of e-8\n",
        "print('Testing relu_forward function:')\n",
        "print('difference: ', rel_error(out, correct_out))"
      ],
      "metadata": {
        "id": "wM5rcQoJ0tDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - dout: Upstream derivatives, of any shape\n",
        "    - cache: Input x, of same shape as dout\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    dx, x = None, cache\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU backward pass.                                 #\n",
        "    ###########################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    pass\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx\n"
      ],
      "metadata": {
        "id": "Yz15y2ms0dCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(231)\n",
        "x = np.random.randn(10, 10)\n",
        "dout = np.random.randn(*x.shape)\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
        "\n",
        "_, cache = relu_forward(x)\n",
        "dx = relu_backward(dout, cache)\n",
        "\n",
        "# The error should be on the order of e-12\n",
        "print('Testing relu_backward function:')\n",
        "print('dx error: ', rel_error(dx_num, dx))"
      ],
      "metadata": {
        "id": "lPcJGEh20wTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax with Loss Layer"
      ],
      "metadata": {
        "id": "OvPcKoP21SZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_loss(x, y):\n",
        "    \"\"\"\n",
        "    Computes the loss and gradient for softmax classification.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
        "      class for the ith input.\n",
        "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
        "      0 <= y[i] < C\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss: Scalar giving the loss\n",
        "    - dx: Gradient of the loss with respect to x\n",
        "    \"\"\"\n",
        "    loss, dx = None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the loss and gradient for softmax classification. This  #\n",
        "    # will be similar to the softmax loss vectorized implementation in        #\n",
        "    # cs231n/classifiers/softmax.py.                                          #\n",
        "    ###########################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    pass\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return loss, dx"
      ],
      "metadata": {
        "id": "PqcmNv_-0Zs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(231)\n",
        "num_classes, num_inputs = 10, 50\n",
        "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
        "y = np.random.randint(num_classes, size=num_inputs)\n",
        "\n",
        "\n",
        "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
        "loss, dx = softmax_loss(x, y)\n",
        "\n",
        "# Test softmax_loss function. Loss should be close to 2.3 and dx error should be around e-8\n",
        "print('\\nTesting softmax_loss:')\n",
        "print('loss: ', loss)\n",
        "print('dx error: ', rel_error(dx_num, dx))"
      ],
      "metadata": {
        "id": "_FVJFNew012i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "HgL9uio108X_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BONUS: SVM Loss \n",
        "Try if you have some time"
      ],
      "metadata": {
        "id": "DVIil6wE1XMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def svm_loss(x, y):\n",
        "    \"\"\"\n",
        "    Computes the loss and gradient using for multiclass SVM classification.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
        "      class for the ith input.\n",
        "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
        "      0 <= y[i] < C\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss: Scalar giving the loss\n",
        "    - dx: Gradient of the loss with respect to x\n",
        "    \"\"\"\n",
        "    loss, dx = None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement loss and gradient for multiclass SVM classification.    #\n",
        "    # This will be similar to the svm loss vectorized implementation in       #\n",
        "    # cs231n/classifiers/linear_svm.py.                                       #\n",
        "    ###########################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    pass\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return loss, dx"
      ],
      "metadata": {
        "id": "V2o-ABI90cYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dx_num = eval_numerical_gradient(lambda x: svm_loss(x, y)[0], x, verbose=False)\n",
        "loss, dx = svm_loss(x, y)\n",
        "\n",
        "# Test svm_loss function. Loss should be around 9 and dx error should be around the order of e-9\n",
        "print('Testing svm_loss:')\n",
        "print('loss: ', loss)\n",
        "print('dx error: ', rel_error(dx_num, dx))"
      ],
      "metadata": {
        "id": "mJjXrBV21gtv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
